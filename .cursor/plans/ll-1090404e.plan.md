<!-- 1090404e-82e1-40a6-a5ab-29efd05a6a43 12d11469-f927-4eb1-9911-4e7ac27f0673 -->
### LLM-Based Test & Debug Helpers Plan

#### 1. Generic test-case generation helper (code → tests)

- **New module**: add a core helper (e.g. `mcp/core/test_generation_llm.py`) that is *not* tied to the current e-commerce CFG.
- **Pydantic models**: define generic models like `GeneratedTestCase` (fields such as `name`, `description`, `input`, `expected_output`, optional `notes`) and `GeneratedTestSuite` (e.g. `target_function`, `summary`, `tests: list[GeneratedTestCase]`).
- **Gemini agent setup**: create a `pydantic_ai.Agent('google-gla:gemini-2.5-pro')` using the existing `pydantic-ai-slim[google] `dependency and `GOOGLE_API_KEY` env, following the Google configuration docs.
- **Prompt design**: implement `build_test_gen_prompt(code_snippet: str, maybe_context: str | None)` that:
- Includes the raw code snippet.
- Asks Gemini to infer likely inputs/outputs and edge cases, without any domain-specific assumptions.
- Requests output conforming strictly to the `GeneratedTestSuite` schema.
- **Main API**: expose `generate_tests_for_code(code_snippet: str, context: str | None = None) -> GeneratedTestSuite` that calls the agent’s `run_sync` with `output_type=GeneratedTestSuite` and returns the structured result.

#### 2. Debug prompt + per-block assessment helper

- **Pydantic models**: in a second module (e.g. `mcp/core/debug_analysis_llm.py`), define models for:
- `FailedTest` (e.g. `input`, `expected`, `actual`, optional `name/notes`).
- `RuntimeStateSnapshot` (e.g. `before: dict[str, object]`, `after: dict[str, object]`, optional `block_id` or index).
- `BlockInfo` (e.g. `id`, `code`, optional `file_path`, `start_line`, `end_line`).
- `BlockAssessment` (e.g. `block: str`, `correct: bool`, `explanation: str`).
- `DebugAnalysis` (e.g. `task_description`, `failed_test`, `assessments: list[BlockAssessment]`).
- **Prompt builder**: implement `build_debug_prompt(task_description, blocks, runtime_states, failed_test)` following your sketch:
- Embed the failing test’s input/expected/actual.
- For each block, format `before`/`after` vars plus the block’s `code` into a `[BLOCK-i]` section.
- Instruct the model to “Respond with JSON for each block: {"block": "BLOCK-X", "correct": true/false, "explanation": "..."}” and to match `DebugAnalysis` / `BlockAssessment` exactly.
- **Gemini agent usage**: reuse or create a `debug_agent = Agent('google-gla:gemini-2.5-pro')` (can share with the test generator) and call `run_sync` with `output_type=DebugAnalysis`.
- **Top-level API**: expose `analyze_failed_test(task_description, blocks, runtime_states, failed_test) -> DebugAnalysis` that:
- Builds the prompt with `build_debug_prompt`.
- Calls Gemini, parses into `DebugAnalysis`, and returns structured per-block correctness judgments.

#### 3. Central `agent.py` encapsulating LLM behavior

- **New file**: introduce `mcp/core/agent.py` containing a single class (e.g. `LlmDebugAgent`) that owns all pydantic-ai / Gemini interaction.
- **Responsibilities**:
- Construct and hold a `pydantic_ai.Agent` configured with `google-gla:gemini-2.5-pro` (using `GOOGLE_API_KEY` via `GoogleProvider` as per the Google configuration docs).
- Expose domain-level methods like `generate_tests_for_code(...)` and `analyze_failed_test(...)` that:
  - Accept plain Python data (code snippets, CFG blocks, runtime snapshots, failed test info).
  - Internally build prompts (`build_test_gen_prompt`, `build_debug_prompt`).
  - Call `run_sync(..., output_type=...)` with the appropriate Pydantic models and return typed results.
- Keep the class mostly stateless (no per-request mutable fields) so it’s safe to reuse across requests and easy to test.
- **Integration points**: other modules (e.g. `mcp/core/mcp_tools.py`, runtime tracer, HTTP/MCP endpoints) depend only on `LlmDebugAgent` instead of directly on pydantic-ai or Gemini config, making it easy to swap models or add logging.

#### 4. Standalone integration / demo wiring module

- **Separate file**: create a dedicated module (e.g. `mcp/core/llm_workflow_demo.py`) that wires together `LlmDebugAgent`, the CFG/runtime pieces, and simple entrypoints for manual testing (CLI function or small helper functions), **without touching existing API route files**.
- **Responsibilities**:
- Provide thin functions like `demo_generate_tests_for_code()` and `demo_analyze_failed_test()` that:
  - Import and instantiate `LlmDebugAgent`.
  - Call `generate_tests_for_code(...)` and `analyze_failed_test(...)` with hard-coded or injectable examples.
  - Print or return results in a JSON-friendly way so they’re easy to inspect.
- Serve as a sandbox for iterating on the LLM workflow independent of whoever is implementing the HTTP / MCP routes.
- **Future integration**: once the workflow is stable, the separate API/routes implementation can call into `LlmDebugAgent` directly rather than redoing wiring, using this demo module as a reference.

### To-dos

- [ ] Design generic Pydantic models for test generation and debug analysis (tests, failed test, runtime snapshot, per-block assessment).
- [ ] Implement the Gemini-based `generate_tests_for_code` helper with a domain-agnostic prompt and structured `GeneratedTestSuite` output.
- [ ] Implement the debug prompt builder and `analyze_failed_test` helper that take blocks, runtime states, and a failed test, and return structured `DebugAnalysis`.
- [ ] Implement central `LlmDebugAgent` in `mcp/core/agent.py` that wraps the shared pydantic-ai Agent and exposes `generate_tests_for_code` / `analyze_failed_test` methods.
- [ ] Create a standalone `mcp/core/llm_workflow_demo.py` module that wires `LlmDebugAgent` into simple demo functions for generating tests and analyzing failed tests, without changing existing API routes.