<!-- 369c7609-c827-4d29-84ec-e90d06ed81a2 de44214b-fad5-4aeb-8541-389c82db9509 -->
# Wire LLM-Generated Tests Into CFG Tracer & Debug Analysis

#### Goals

- Use `LlmDebugAgent.generate_tests_for_code` to produce tests for CFG code.
- Run one or more generated tests through the existing block tracer.
- Transform trace output into `BlockInfo` + `RuntimeStateSnapshot` and feed it to `analyze_failed_test`.
- Keep regeneration/patching out of scope for now.

#### Plan

1. **Review existing pieces and constraints**

- Confirm current models and helpers: `GeneratedTestSuite` / `GeneratedTestCase` (`test_generation_llm.py`), `FailedTest` / `RuntimeStateSnapshot` / `BlockInfo` / `DebugAnalysis` (`debug_analysis_llm.py`), `BasicBlock` / `TraceEntry` / tracer + runner (`debug_types.py`, `runtime_tracer.py`, `block_trace_runner.py`), and dummy CFG + sources (`dummy_cfg.py`).
- Verify how `build_runner_payload` and `run_with_block_tracing_subprocess` are used in `mcp_tools.py` so the new wiring reuses that contract.

2. **Define an orchestration module for the end-to-end flow**

- Create a new core module (e.g. `mcp/core/llm_workflow_orchestrator.py`) that will:
- Accept a `LlmDebugAgent` and a `task_description`.
- Optionally accept custom `sources` and `BasicBlock` lists so it can later plug into the real CFG instead of `dummy_cfg`.
- Expose a single high-level function like `run_generated_test_through_tracer_and_analyze(...)` returning a small dataclass bundling `GeneratedTestSuite`, the chosen `GeneratedTestCase`, raw trace payload, and `DebugAnalysis`.

3. **Render a `GeneratedTestCase` into executable Python test code**

- In the orchestrator, implement a helper (e.g. `render_generated_test_case_to_python(case, suite) -> str`) that:
- Treats `case.input` as Python code that sets up inputs and calls the target function.
- Treats `case.expected_output` as Python assertions over the result.
- Wraps them in a small comment header with the test name and `suite.target_function` for debugging.
- If needed, slightly adjust the test-generation prompt (in `build_test_gen_prompt`) later so the LLM reliably produces code compatible with this renderer.

4. **Bridge GeneratedTestSuite → tracer payload**

- Inside `run_generated_test_through_tracer_and_analyze`:
- Call `agent.generate_tests_for_code(code_snippet=...)` using a representative source snippet (initially first entry from `get_dummy_sources()`).
- Select a `GeneratedTestCase` (initially index 0, with room to extend to different selection strategies later).
- Use the renderer to build the `tests` string.
- Build the tracer payload via `build_runner_payload(sources=..., blocks=..., tests=tests_code)` using either dummy or injected CFG data.
- Call `run_with_block_tracing_subprocess(payload=...)` and capture `trace`, `ok`, and `error`.

5. **Map trace entries to `BlockInfo` + `RuntimeStateSnapshot`**

- Implement helpers in the orchestrator to:
- Convert `BasicBlock` + `sources` into `BlockInfo` with real code snippets by slicing `entry['code'] `lines between `start_line` and `end_line`.
- Walk sorted `trace` (`TraceEntry` dicts) and, for each `block_id`, build a first-execution `RuntimeStateSnapshot` with `before` (locals from previous step, if any) and `after` (locals from the current step), dropping blocks that never executed so lengths match.
- Ensure resulting `blocks: list[BlockInfo]` and `runtime_states: list[RuntimeStateSnapshot] `are aligned one-to-one for `build_debug_prompt`.

6. **Construct `FailedTest` from test + runtime outcome**

- Build a `FailedTest` instance using:
- `name` from the `GeneratedTestCase`.
- `input` from `case.input`.
- `expected` from `case.expected_output`.
- `actual` from the tracer result: if `error` exists, use its `message` (and optionally `traceback` in `notes`), otherwise set a simple string like "All assertions passed (no error)".

7. **Call `analyze_failed_test` and return a structured result**

- Invoke `agent.analyze_failed_test(task_description, blocks, runtime_states, failed_test)` with the reconstructed structures.
- Wrap `suite`, the chosen `test_case`, the raw `trace_payload`, and `debug_analysis` in a small dataclass (e.g. `LlmDebugRunResult`) so callers (demos, future routes) get all artifacts in one object.

8. **Integrate with the demo module for manual testing**

- In `mcp/demos/llm_workflow_demo.py`, add a new function (e.g. `demo_full_llm_debug_from_generated_test`) that:
- Instantiates `LlmDebugAgent`.
- Calls `run_generated_test_through_tracer_and_analyze(...)` with a simple `task_description`.
- Pretty-prints the suite, selected test, raw trace JSON, and `DebugAnalysis` JSON for inspection.
- Keep existing demos intact so you can compare synthetic vs. real-trace behavior.

9. **Future hooks (not implemented now, but keep in mind)**

- Later, swap `get_dummy_sources` / `get_dummy_blocks` for the real CFG builder output when available.
- Add support for iterating over *all* generated tests and aggregating analyses.
- Introduce a regeneration API on `LlmDebugAgent` that takes `DebugAnalysis` and produces code patches, mirroring the final step in the LDB paper.

### To-dos

- [ ] Review existing models and helpers in test_generation_llm.py, debug_analysis_llm.py, debug_types.py, runtime_tracer.py, block_trace_runner.py, dummy_cfg.py, and mcp_tools.py to confirm contracts.
- [ ] Create a new orchestration module (e.g. mcp/core/llm_workflow_orchestrator.py) with a top-level function to run a generated test through the tracer and debug analysis.
- [ ] Implement a helper to render a GeneratedTestCase + GeneratedTestSuite into executable Python test code string.
- [ ] Inside the orchestrator, call generate_tests_for_code, pick a test, render it, and build a tracer payload using build_runner_payload and run_with_block_tracing_subprocess.
- [ ] Implement helpers to convert BasicBlock + sources + trace entries into aligned BlockInfo and RuntimeStateSnapshot sequences.
- [ ] Construct a FailedTest object from the chosen GeneratedTestCase and tracer error/ok status.
- [ ] Invoke agent.analyze_failed_test with the reconstructed structures and wrap results in a small dataclass.
- [ ] Add a demo function in mcp/demos/llm_workflow_demo.py to exercise the full LLM-generated-test → tracer → debug-analysis flow and print results.